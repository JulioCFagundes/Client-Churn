# -*- coding: utf-8 -*-
"""CustomerChurning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HkJg8gKEXnFbVrBmS9vkM_lYlHQ3EFd-

**Este é um trabalho realizado por alunos da UFPR para a matéria de graduação de Machine Learning. A database utilizada foi a Telco Customer Churn, disponível em https://www.kaggle.com/datasets/blastchar/telco-customer-churn.**

**Nosso objetivo é criar um modelo de machine learning que consiga prever se um cliente irá ou não dar o Churn utilizando os dados fornecidos pela database.**

**Para este trabalho, utilizamos a validação hold-out com 70% dos dados para treino. Além disso, como será visto durante o código, para filtragem de variáveis foi utilizada a regressão lasso, com a qual fizemos um grid-search e também comparamos resultados para diferentes alphas.**

**Trouxemos também algumas reflexões e análises sobre regras de negócios, análise de covariância, balanceamento e transformação de dados.**

**Os modelos de treinamento utilizados foram KNN, Naive Bayes, Random Forest e xgboost com parâmetros determinados com grid search.**

**Tendo em vista a natureza do trabalho, nosso objetivo não é fazer um algoritmo estado de arte, mas analisar e comparar resultados, trazendo reflexões tanto para nós, autores, quanto para os futuros leitores.**
"""

import sklearn
import pandas  as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from google.colab import files
from sklearn.model_selection import cross_val_score
from sklearn.metrics import ConfusionMatrixDisplay
from imblearn.over_sampling import RandomOverSampler
from xgboost import XGBClassifier, plot_importance
from sklearn.model_selection import GridSearchCV

files.upload()

"""**LENDO O ARQUIVO CSV**"""

dataset = pd.read_csv('dataset.csv')
dataset.head()

"""**NO PRIMEIRO COMANDO, ESTAMOS FAZENDO A CONTAGEM DE CAMPOS TOTAIS, CAMPOS NULOS E OBSERVANDO A TIPAGEM DE CADA FEATURE.**

**NO SEGUNDO, PODEMOS OBSERVAR, ALÉM DA CONTAGEM DOS CAMPOS, A MÉDIA, DESVIO PADRÃO, O MÍNIMO E O MÁXIMO DAS FEATURES NUMÉRICAS**
"""

## printa as informações do dataset. Nosso objetivo é saber o tipo das variáveis e verrificar se é necessária alguma alterção
dataset.info()

## printa a quantidade de linhas preenchidas, média, desvio padrão e menor valor
dataset.describe()

"""**Neste primeiro treino vamos fazer a validação hold-out, que consiste em dividir base de dados entre base de treino e base de validação. Nossa escolha de proporção foi de 70% dos dados para treinamento e 30% dos dados para validação.**"""

base_treino = dataset.sample(frac=0.7,random_state=42) # Pegando 70% da base aleatóriamente para ser treino.
print(base_treino.shape)

index_fora_do_treino = dataset.index.difference(base_treino.index) # Pegando os indexes que não estão na base

base_val = dataset.iloc[index_fora_do_treino]
print(base_val.shape)

"""**Agora nós temos a base de treino com 4930 linha e a base de validação com 2113, totalizando os 7043 clientes únicos.**

Dividindo as features da resposta
"""

X_treino = base_treino.drop(columns='Churn')
Y_treino = base_treino['Churn']
X_val = base_val.drop(columns='Churn')
Y_val = base_val.Churn

"""Agora, vamos verificar o balanceamento do dataset:"""

Churn = base_treino[base_treino.Churn == 'Yes'].customerID.count()
NoChurn = base_treino[base_treino.Churn == 'No'].customerID.count()

ChurnPercentage = Churn/(Churn + NoChurn)

print(f'Churn percentage = {ChurnPercentage}')

"""Apenas 26,9% dos clientes deram Churn neste dataset. Para evitar problemas com o treinamento, realizaremos um oversampling para que a classe minoritária alcance 50% do dataset. (número alcançado a partir de testes manuais)

Como nossos dados estão desbalanceados, utilizaremos a biblioteca imblearn para realizar um oversampling
"""

ros = RandomOverSampler(sampling_strategy=1,random_state=42)
X_resampled, y_resampled = ros.fit_resample(X_treino, Y_treino)

"""Vamos verificar o tamanho do dataset e se o oversample foi realizado corretamente."""

Resample_df = X_resampled
Resample_df['Churn'] = y_resampled

Churn = Resample_df[Resample_df.Churn == 'Yes'].customerID.count()
NoChurn = Resample_df[Resample_df.Churn == 'No'].customerID.count()

ChurnPercentage = Churn/(Churn + NoChurn)
print(f'Churn percentage = {ChurnPercentage}')

"""**Agora, com um Churn rate de 50%, vamos tratar os dados de treino. Lembrando que tudo que for feito nos dados de treino, também deve ser feito nos dados de validação.**

# **DADOS DE TREINO**
"""

X_treino = X_resampled
Y_treino = y_resampled

"""**Primeiramente, analisaremos os dados de treino utilizando os comandos info e describe.**"""

X_treino.info()
X_treino.describe()

"""**Note que a feature "Total Charges" está tipada como objeto. Portanto, faremos uma conversão para float.**"""

#Transforma a coluna TotalCharges em numérica, o parâmetro coerce força com que a transformação seja feita, ignorando os erros.
X_treino["TotalCharges"] = pd.to_numeric(X_treino.TotalCharges, errors='coerce')

"""**Tendo em vista que mudamos a tipagem da coluna TotalCharges de object para float utilizando o parâmetro "coerce", que força essa mudança, é interessante analisarmos novamente se esta coluna tem algum dado NaN (not a number), que poderá influenciar futuramente a nossa modelagem.**



"""

X_treino[X_treino.TotalCharges.isna()]

"""Note que exatamente os mesmos clientes que estão com dados faltantes no campo TotalCharges são os únicos com tenure igual a zero."""

X_treino[X_treino.tenure == 0]

"""**Encontramos seis clientes onde a feature "TotalCharges" está faltante. Note que o comando describe não captou estes dados. Isso se deve ao fato de termos strings com um espaço no campo, fazendo com que o describe entendesse aquilo como um campo preenchido. Mas quando trocamos para float e utilizamos o comando "coerce" os campos nulos são evidenciados**


**Para resolver o problema dos campos nulos, vamos fazer uma análise dos dados obtidos.**

**No banco de dados, nós temos três colunas que podem nos ajudar a entender estes dados faltantes. A coluna tenure, que é o período em que o cliente consome (ou consumiu) o nosso produto, em meses; a coluna monthly charges, que corresponde a conta mensal que deve ser paga pelo cliente pelo serviço e finalmente a coluna totalcharges, que corresponde ao montante total pago pelo cliente até o momento.**

**Portanto, podemos supor que a coluna totalcharges possa ser descrita da seguinte maneira:**



>>>TotalCharges = (MonthlyCharges) x (Tenure)

**Porém, em algumas linhas podemos ver sutis diferenças entre o valor de TotalCharges calculado desta maneira e o real. Porém, para o caso de Tenure igual a zero, ou seja, o cliente ainda não completou um mês, podemos entender que a primeira fatura ainda não foi cobrada e, portanto, TotalCharges é igual a zero neste momento. Exatamente o que acontece nas linhas com dados faltantes, todas elas tem tenure igual a zero. Portanto, podemos fazer esta interpretação dos dados**






"""

X_treino['TotalCharges'].replace(np.NaN, 0, inplace=True)
X_treino[X_treino.TotalCharges.isna()]

"""**Agora, sem dados faltantes, podemos continuar a nossa modelagem. O primeiro passo será remover a coluna de Id dos clientes para evitar dados vazados.**

"""

X_treino.head()
X_treino.drop('customerID', axis=1, inplace=True)

"""**Transformando os dados de classificação em numéricos:**"""

for i in X_treino:

  if X_treino[i].dtypes=='object':
    print(f'{i}: {X_treino[i].unique()}' )



#Filtrando respostas com mesmo sentido
X_treino.replace('No phone service', 'No', inplace=True)
X_treino.replace('No internet service', 'No', inplace=True)

#Transformando os dados de classificação em numéricos
X_treino_num = X_treino.replace('No', 0)
X_treino_num.replace('Yes', 1, inplace=True)
X_treino_num['gender'].replace('Female', 0, inplace=True)
X_treino_num['gender'].replace('Male', 1, inplace=True)
X_treino_num.replace('DSL', 1, inplace=True)
X_treino_num.replace('Fiber optic', 2, inplace=True)
X_treino_num.replace('Month-to-month', 0, inplace=True)
X_treino_num.replace('Two year',  2, inplace=True)
X_treino_num.replace('One year', 1, inplace=True)
X_treino_num.replace('Electronic check', 0, inplace=True)
X_treino_num.replace('Bank transfer (automatic)', 1, inplace=True)
X_treino_num.replace('Mailed check', 2, inplace=True)
X_treino_num.replace('Credit card (automatic)', 3, inplace=True)

"""Agora podemos olhar para a correlação e variância das features"""

plt.subplots(figsize=(20,8))

df_plot = X_treino_num

df_plot['Churn'] = np.where(Y_treino == 'No', 0, 1)
features_correlation=df_plot.corr()
sb.heatmap(features_correlation,annot=True,cmap='RdPu')
plt.title('Correlation between the variables')
plt.xticks(rotation=45)

sb.violinplot(data=X_treino_num[['MonthlyCharges', 'tenure']], palette="Set3", bw=.2, cut=1, linewidth=1)

sb.violinplot(data=X_treino_num[['TotalCharges']], palette="Set3", bw=.2, cut=1, linewidth=1)

plot_dataframe = X_treino_num.drop(columns=['TotalCharges', 'MonthlyCharges', 'tenure'])
Y_treino = np.where(Y_treino == 'No', 0, 1)

df = pd.melt(plot_dataframe, plot_dataframe.columns[1], plot_dataframe.columns[:])

g = sb.FacetGrid(df, col="variable", col_wrap=5)
g.map(sb.histplot, "value", linewidth=2)

X_treino_num.drop(columns='Churn', inplace=True)

"""**Normalizando os dados para evitar problemas de escala no modelo**"""

def normalizar(lista):
    return (lista - np.min(lista))/(np.max(lista) - np.min(lista))

X_train_norm = X_treino_num.apply(lambda x: normalizar(x), axis = 0)
X_train_norm.head()

"""USANDO REGRESSÃO LASSO PARA FILTRAR AS VARIÁVEIS NORMALIZADAS"""

from sklearn import linear_model

coef_matrix = []
search_space = {
    "alpha" : [0.00001, 0.00005, 0.0001,0.0005, 0.001,0.004, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.15]
}
for i in range(len(search_space["alpha"])):
  clf = linear_model.Lasso(alpha=search_space["alpha"][i])
  clf.fit(X_train_norm, Y_treino)
  coef_matrix.append(clf.coef_)
  print(coef_matrix)

"""Para entender melhor, vamos fazer um gráfico dessa matriz de coeficientes"""

alpha_index = []
for i in range(len(search_space['alpha'])):
  alpha_index.append(f'alpha{i}')
alphas = pd.DataFrame(search_space['alpha'], index=alpha_index)

coef_dataframe = pd.DataFrame(coef_matrix, index=search_space['alpha'])

coef_dataframe

plt.plot(coef_dataframe)

coef_dataframe.loc[0.03]

coef_dataframe.loc[0.04]

"""Para alpha = 0.03 o filtro da regressão forneceu 3 betas diferentes de zero, para alpha = 0.02 temos 7 features e para alpha = 0.01 temos 9 features. Assim, acreditamos que esse seja um bom ponto de corte para realizar os testes, tendo em vista que para valores maiores temos um número muito pequeno de betas e não queremos fazer previsões baseadas em somente uma feature.

TREINAMENTO COM TODAS AS FEATURES

Vamos testar os modelos de classificação kNN, Naive Bayes, Random Forest e xgboost

Tratar os dados de validação pra testar o modelo
"""

## Transformando os valores da coluna TotalCharges em numéricos
X_val["TotalCharges"] = pd.to_numeric(X_val.TotalCharges, errors='coerce')
X_val['TotalCharges'].replace(np.NaN, 0, inplace=True)

##Tirando a coluna customerID
X_val.drop(columns='customerID', inplace=True)

## Transformando os dados de classificação em numéricos
X_val.replace('No phone service', 'No', inplace=True)
X_val.replace('No internet service', 'No', inplace=True)
X_val.replace('No', 0, inplace=True)
X_val.replace('Yes', 1, inplace=True)
X_val.replace('Female', 0, inplace=True)
X_val.replace('Male', 1, inplace=True)
X_val.replace('DSL', 1, inplace=True)
X_val.replace('Fiber optic', 2, inplace=True)
X_val.replace('Month-to-month', 0, inplace=True)
X_val.replace('Two year',  2, inplace=True)
X_val.replace('One year', 1, inplace=True)
X_val.replace('Electronic check', 0, inplace=True)
X_val.replace('Bank transfer (automatic)', 1, inplace=True)
X_val.replace('Mailed check', 2, inplace=True)
X_val.replace('Credit card (automatic)', 3, inplace=True)

## Escalando o dataset
X_val_norm = X_val.apply(lambda x: normalizar(x), axis = 0)

## Transformando a resposta de classificação em numérica
Y_val.replace('Yes', 1, inplace=True)
Y_val.replace('No', 0, inplace=True)

"""TREINO COM KNN"""

from sklearn.neighbors import KNeighborsClassifier
# definindo a quantidade de vizinhos
neigh = KNeighborsClassifier(n_neighbors=3)

param_grid = {'n_neighbors': np.arange(2,30,1)}
grid_search = GridSearchCV(neigh, param_grid, scoring='recall')
grid_result = grid_search.fit(X_train_norm, Y_treino)

print(f'Best result: {grid_result.best_score_} for {grid_result.best_params_}')

## Ajustando os dados que vão para treino
neigh.fit(X_train_norm, Y_treino)
predict = list(neigh.predict(X_val_norm))

## Fazendo a matriz confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)
ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

"""Validação"""

from sklearn.metrics import recall_score
recall_score(Y_val, predict)

"""Treino"""

predict = neigh.predict(X_train_norm)
recall_score(Y_treino, predict)

"""TREINO UTILIZANDO AS 6 VARIÁVEIS FILTRADOS NA REGRESSÃO LASSO COM ALPHA = 0,03


"""

## Dropando as colunas que ficaram com Beta = 0
X_train_norm_alpha_um = X_train_norm[['tenure', 'InternetService', 'OnlineSecurity', 'Contract', 'PaperlessBilling', 'PaymentMethod']]
X_val_norm_alpha_um = X_val_norm[['tenure', 'InternetService', 'OnlineSecurity', 'Contract', 'PaperlessBilling', 'PaymentMethod']]

## Ajustando os dados que vão para treino
neigh.fit(X_train_norm_alpha_um, Y_treino)
predict = list(neigh.predict(X_val_norm_alpha_um))

## Fazendo a matriz confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)
ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

"""Validação"""

recall_score(Y_val, predict)

"""Treino"""

predict = neigh.predict(X_train_norm_alpha_um)
recall_score(Y_treino, predict)

"""TREINO UTILIZANDO AS 2 VARIÁVEIS FILTRADOS NA REGRESSÃO LASSO COM ALPHA = 0,04


"""

X_train_norm_alpha_dois = X_train_norm[['InternetService', 'Contract']]
X_val_norm_alpha_dois = X_val_norm[[ 'InternetService', 'Contract']]

## Ajustando os dados que vão para treino
neigh.fit(X_train_norm_alpha_dois, Y_treino)
predict = list(neigh.predict(X_val_norm_alpha_dois))

## Fazendo a matriz confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)
ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

"""Validação"""

recall_score(Y_val, predict)

"""Treino"""

predict = neigh.predict(X_train_norm_alpha_dois)
recall_score(Y_treino, predict)

"""TREINO COM NAIVE BAYES

Com todas as variáveis
"""

from sklearn.naive_bayes import MultinomialNB

clf = MultinomialNB()
clf.fit(X_train_norm, Y_treino)
predict = clf.predict(X_val_norm)

##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

recall_score(Y_val, predict)

"""Com 6 variáveis"""

clf = MultinomialNB()
clf.fit(X_train_norm_alpha_um, Y_treino)
predict = clf.predict(X_val_norm_alpha_um)

##fazendo a matriz de
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

recall_score(Y_val, predict)

"""Com 2 variáveis"""

clf = MultinomialNB()
clf.fit(X_train_norm_alpha_dois, Y_treino)
predict = clf.predict(X_val_norm_alpha_dois)

##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

recall_score(Y_val, predict)

"""TREINO COM RANDOM FOREST"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(n_estimators = 1200)
param_grid = {'n_estimators': np.arange(500,2000,100)}
grid_search = GridSearchCV(clf, param_grid, scoring='recall')
grid_result = grid_search.fit(X_train_norm, Y_treino)

print(f'Best result: {grid_result.best_score_} for {grid_result.best_params_}')

clf = RandomForestClassifier(n_estimators = 1200)

clf.fit(X_train_norm, Y_treino)

predict = clf.predict(X_val_norm)

##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

"""Validação"""

recall_score(Y_val, predict)

"""Treino"""

predict = clf.predict(X_train_norm)
recall_score(Y_treino, predict)

"""Com 6 variáveis"""

clf = RandomForestClassifier(n_estimators = 900)
clf.fit(X_train_norm_alpha_um, Y_treino)
predict = clf.predict(X_val_norm_alpha_um)

##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

"""Validação"""

recall_score(Y_val, predict)

"""Treino"""

predict = clf.predict(X_train_norm_alpha_um)
recall_score(Y_treino, predict)

"""Com 2 variáveis"""

clf = RandomForestClassifier(n_estimators = 900)
clf.fit(X_train_norm_alpha_dois, Y_treino)
predict = clf.predict(X_val_norm_alpha_dois)

##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

"""Validação"""

recall_score(Y_val, predict)

"""Treino

"""

predict = clf.predict(X_train_norm_alpha_dois)
recall_score(Y_treino, predict)

"""TREINO XGBOOST COM TODAS AS VARIÁVEIS"""

xgb = XGBClassifier()

# parameter to be searched
param_grid = {'learning_rate': [0.0001, 0.01, 0.1, 1]}

# find the best parameter
grid_search = GridSearchCV(xgb, param_grid, scoring='recall')
grid_result = grid_search.fit(X_train_norm, Y_treino)

print(f'Best result: {grid_result.best_score_} for {grid_result.best_params_}')

# parameter to be searched
param_grid = { 'gama': np.arange(0.0,20.0,0.05)}

# find the best parameter
grid_search = GridSearchCV(xgb, param_grid, scoring='recall')
grid_result = grid_search.fit(X_train_norm, Y_treino)

print(f'Best result: {grid_result.best_score_} for {grid_result.best_params_}')

# parameter to be searched
param_grid = {'max_depth': range(1,8,1),
              'min_child_weight': np.arange(0.0001, 0.5, 0.01)}

# find the best parameter
grid_search = GridSearchCV(xgb, param_grid, scoring='recall')
grid_result = grid_search.fit(X_train_norm, Y_treino)

print(f'Best result: {grid_result.best_score_} for {grid_result.best_params_}')

xgb = XGBClassifier(learning_rate=0.0001, n_estimators=25, max_depth=1, min_child_weight=0.0001, gamma=0)
xgb.fit(X_train_norm, Y_treino)

predict = xgb.predict(X_val_norm)
##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

recall_score(Y_val, predict)

"""COM 6 VARIÁVEIS"""

xgb = XGBClassifier(learning_rate=0.0001, n_estimators=25, max_depth=1, min_child_weight=0.0001, gamma=0)
xgb.fit(X_train_norm_alpha_um, Y_treino)

predict = xgb.predict(X_val_norm_alpha_um)
##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

recall_score(Y_val, predict)

"""COM 2 VARIÁVEIS"""

xgb = XGBClassifier(learning_rate=0.0001, n_estimators=25, max_depth=1, min_child_weight=0.0001, gamma=0)
xgb.fit(X_train_norm_alpha_dois, Y_treino)

predict = xgb.predict(X_val_norm_alpha_dois)
##fazendo a matriz de confusão
confusion_matrix = sklearn.metrics.confusion_matrix(Y_val, predict)

ax= plt.subplot()
sb.heatmap(confusion_matrix, xticklabels=['Não Churn', 'Churn'], yticklabels=['Não Churn', 'Churn'], annot=True, fmt='g', ax=ax);

ax.set_xlabel('Predicted');ax.set_ylabel('True');
ax.set_title('Confusion Matrix');

"""validação"""

predict = clf.predict(X_val_norm_alpha_dois)
recall_score(Y_val, predict)

"""Treino"""

predict = clf.predict(X_train_norm_alpha_dois)
recall_score(Y_treino, predict)

